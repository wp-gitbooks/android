---
number headings: auto, first-level 1, max 6, 1.1
---


# 1 线索

开篇先定义问题，这篇文章主要是解决哪一类的问题，痛点是什么，之前的方案是怎么设计的，现在的技术方案跟以前相比有什么不同，对其他技术线的同学们来说有什么借鉴价值。这里提醒一句，同学们更喜欢看到方案背后的思考和分析判断，其他相关团队可借鉴复用的部分，深度思考和可借鉴性的价值越大



![image-20210506150743401](https://cdn.jsdelivr.net/gh/wp3355168/Typora-Picgo-Gitee/img/20210506150755.png)





![image-20210628150501458](https://cdn.jsdelivr.net/gh/wp3355168/Typora-Picgo-Gitee/img/20210628150501.png)



![](https://cdn.jsdelivr.net/gh/wp3355168/Typora-Picgo-Gitee/img/20210731223143.png)

# 2 概述

## 2.1 稳定性维度

- Crash纬度
- 性能纬度：启动速度、内存、绘制等等优化方向，相对于Crash来说是次要的
- 业务高可用纬度



## 2.2 稳定性优化

- 重在预防、监控必不可少
- 思考更深一层、重视隐含信息：如解决Crash问题时思考是否会引发同一类问题
- 长效保持需要科学流程



## 2.3 Crash相关指标

### 2.3.1 UV、PV

- PV（Page View）：访问量
- UV（Unique Visitor）：独立访客，0 - 24小时内的同一终端只计算一次


### 2.3.2 UV、PV、启动Crash率

- UV Crash率：针对用户使用量的统计，统计一段时间内所有用户发生崩溃的占比
- Crash UV / DAU：评估Crash率的影响范围，结合PV

注意：沿用同一指标

- PV Crash率：评估相关Crash影响的严重程度
- 启动Crash率：影响最严重的Crash，对用户伤害最大，无法通过热修复拯救，需结合客户端容灾
- 增量、存量Crash率：增量Crash是新版本重点，存量Crash是需要持续啃的硬骨头，优先解决增量、持续跟进存量



## 2.4 Crash率评价

- 必须在千分之二以下
- 万分位为优秀



## 2.5 Crash关键问题

尽可能还原Crash现场:

- 堆栈、设备、OS版本、进程、线程名、Logcat
  前后台、使用时长、App版本、小版本、渠道
  CPU架构、内存信息、线程数、资源包信息、用户行为日志
- Crash现场信息、Crash Top机型、OS版本、分布版本、区域
  Crash起始版本、上报趋势、是否新增、持续、量级
  根据以上信息决定Crash是否需要立马解决以及在哪个版本进行解决
- 参考Bugly平台的APM后台聚合展示



## 2.6 APM Crash部分整体架构

### 2.6.1 采集层

- 错误堆栈
- 设备信息
- 行为日志
- 其它信息

### 2.6.2 处理层

- 数据清洗
- 数据聚合
- 纬度分类
- 趋势对比

### 2.6.3 展示层

- 数据还原
- 纬度信息
- 起始版本
- 其它信息

### 2.6.4 报警层

- 环比：期与上一期进行对比
- 同比：如本月10号与上月10号
- 邮件
- IM
- 电话

### 2.6.5 责任归属

- 设立专项小组轮值
- 自动匹配责任人
- 处理流程全纪录



## 2.7 问题类型

### 2.7.1 有稳定复现路径的问题

- 如果一个问题能有稳定的复现路径，解决问题就变得容易很多。
- 这类问题不论Debug信息的多少，都能找根本原因。因为能复现，就可以不断的增加Debug信息。
- 严格来说这类问题不能称为稳定性问题，但依然占据稳定性问题的多数。

### 2.7.2 偶现问题，且Debug信息足够

- 仅使用Android标准的Debug信息就可以定位问题时，都比较容易解决，也不会太复杂。对系统有很好的理解，并且会分析栈信息可能就够用了。但可能需要分析源码来确定根本原因。
- 需要借助厂商附加的Debug信息定位问题时，问题会相对复杂。
- 面对复杂问题时，可能需要仔细分析不同的Debug信息，在它们之间找到关联。尽可能的搜索相关线索，这样才能找到根本原因。
- 有时可能能根据Debug信息定位到代码段，但真正的根本原因还需要分析代码逻辑才能确定。不要轻易下手修改代码，尤其是原生代码。
- 定位到原生代码出问题时，先在网上找一下是否有人给出Patch，相信你不可能是第一个碰到这种问题的人。
- 编写测试代码来验证你的解决方案。

### 2.7.3 偶现问题，且Debug信息不足

- Debug信息不够用时就麻烦了，这也是我认为最难解决的问题。
- 试图寻找复现路径，让问题变为有稳定复现路径的问题。
- 在怀疑点增加Debug信息，等待下一次复现。
- 根据怀疑点分析代码逻辑，试图使用测试代码来复现该问题。
- 根据core文件中的内存镜像来获取更多的有用信息。
- 仔细分析现有的Debug信息，往往一小句话就有很大的惊喜。
- 注意CPU，Memory，IO的状态对系统的影响。
- 怀疑内存问题时可以使用内存检测工具来检查一遍当前系统隐含的内存问题。
- 有时代码检查工具也能规避很多问题，提交前没有检查的最好检查一下。
- 问题很可能不是第一现场，先解决系统之前的错误，后面的错误可能就跟随解决。
- 同样的，在网上搜索是否有类似问题的解决方案。
- 总体来说，这类问题就是尝试各种方法来解决目前无法解决的问题。

### 2.7.4 不能复现的问题

- 此类问题既然无法复现，也就不重要了。
- 如果可以根据Debug信息解决固然很好，不能解决时可以先降低优先级。
- 等待该问题变成偶先问题时，就可以根据上述方法分析了



# 3 Crash优化

## 3.1 单个Crash处理方案

1、根据堆栈及现场信息找答案

- 解决90%问题
- 解决完后需考虑产生Crash深层次的原因

2、找共性：机型、OS、实验开关、资源包，考虑影响范围

3、线下复现、远程调试

## 3.2 Crash率治理方案

- 1、解决线上常规Crash
- 2、系统级Crash尝试Hook绕过
- 3、疑难Crash重点突破或更换方案

## 3.3 Java Crash

出现未捕获异常，导致出现异常退出

```
Thread.setDefaultUncaughtExceptionHandler()；
```

我们通过设置自定义的UncaughtExceptionHandler，就可以在崩溃发生的时候获取到现场信息。注意，这个钩子是针对单个进程而言的，在多进程的APP中，监控哪个进程，就需要在哪个进程中设置一遍ExceptionHandler。

获取主线程的堆栈信息：

```
Looper.getMainLooper().getThread().getStackTrace();
```

获取当前线程的堆栈信息：

```
Thread.currentThread().getStackTrace();
```

获取全部线程的堆栈信息：

```
Thread.getAllStackTraces();
```

第三方Crash监控工具如Fabric、腾讯Bugly，都是以字符串拼接的方式将数组StackTraceElement[]转换成字符串形式，进行保存、上报或者展示。

### 3.3.1 如何反混淆上传的堆栈信息？

- 每次打包生成混淆APK的时候，需要把Mapping文件保存并上传到监控后台；
- Android原生的反混淆的工具包是retrace.jar，在监控后台用来实时解析每个上报的崩溃时。它会将Mapping文件进行文本解析和对象实例化，这个过程比较耗时。因此可以将Mapping对象实例进行内存缓存，但为了防止内存泄露和内存过多占用，需要增加定期自动回收的逻辑。

### 3.3.2 获取logcat方法

logcat日志流程是这样的，应用层 –> liblog.so –> logd，底层使用ring buffer来存储数据。获取的方式有以下三种：

##### 3.3.2.1.1 1通过logcat命令获取。

- 优点：非常简单，兼容性好。
- 缺点：整个链路比较长，可控性差，失败率高，特别是堆破坏或者堆内存不足时，基本会失败。

##### 3.3.2.1.2 2hook liblog.so实现。通过hook liblog.so 中__android_log_buf_write 方法，将内容重定向到自己的buffer中。

- 优点：简单，兼容性相对还好。
- 缺点：要一直打开。

##### 3.3.2.1.3 3自定义获取代码。通过移植底层获取logcat的实现，通过socket直接跟logd交互。

- 优点：比较灵活，预先分配好资源，成功率也比较高。
- 缺点：实现非常复杂

### 3.3.3 获取Java 堆栈

native崩溃时，通过unwind只能拿到Native堆栈。我们希望可以拿到当时各个线程的Java堆栈。

##### 3.3.3.1.1 Thread.getAllStackTraces()。

优点：简单，兼容性好。

缺点：

- 成功率不高，依靠系统接口在极端情况也会失败。
- 7.0之后这个接口是没有主线程堆栈。
- 使用Java层的接口需要暂停线程。

##### 3.3.3.1.2 hook libart.so。通过hook ThreadList和Thread的函数，获得跟ANR一样的堆栈。为了稳定性，需要在fork的子进程中执行。

- 优点：信息很全，基本跟ANR的日志一样，有native线程状态，锁信息等等。
- 缺点：黑科技的兼容性问题，失败时可以用Thread.getAllStackTraces()兜底



### 3.3.4 Java Crash处理流程

![image](https://cdn.jsdelivr.net/gh/wp3355168/Typora-Picgo-Gitee/img/20210429181421.jpg)



# 4 Native Crash



# 5 崩溃分析流程

首先，应收集崩溃现场的一些信息，如下：

## 5.1 崩溃信息

- 进程名、线程名
- 崩溃堆栈和类型
- 有时候也需要知道主线程的调用栈

## 5.2 系统信息

- 系统运行日志

```
/system/etc/event-log-tags
```

- 机型、系统、厂商、CPU、ABI、Linux版本等

注意寻找共性问题

- 设备状态
- 是否root
- 是否是模拟器

## 5.3 内存信息

**系统剩余内存**

```
/proc/meminfo
```

当系统可用内存小于MemTotal的10%时，OOM、大量GC、系统频繁自杀拉起等问题非常容易出现

**应用使用内存**

包括Java内存、RSS、PSS

PSS和RSS通过/proc/self/smap计算，可以得到apk、dex、so等更详细的分类统计。

**虚拟内存**

大小：

```
/proc/self/status
```

具体分布情况：

```
/proc/self/maps
```

注意：

对于32位进程，32位CPU，虚拟内存达到3GB就可能会引起内存失败的问题。如果是64位的CPU，虚拟内存一般在3~4GB。如果支持64位进程，虚拟内存就不会成为问题。

## 5.4 资源信息

如果应用堆内存和设备内存比较充足，但还出现内存分配失败，则可能跟资源泄漏有关。

**文件句柄fd**

限制数：

```
/proc/self/limits
```

一般单个进程允许打开的最大句柄个数为1024，如果超过800需将所有fd和文件名输出日志进行排查。

**线程数**

大小：

```
/proc/self/status
```

一个线程一般占2MB的虚拟内存，线程数超过400个比较危险，需要将所有tid和线程名输出到日志进行排查。

**JNI**

容易出现引用失效、引用爆表等崩溃。

通过DumpReferenceTables统计JNI的引用表，进一步分析是否出现JNI泄漏等问题。

**补充：dumpReferenceTables的出处**

在dalvik.system.VMDebug类中，是一个native方法，亦是static方法；在JNI中可以这么调用

```
jclass vm_class = env->FindClass("dalvik/system/VMDebug");
jmethodID dump_mid = env->GetStaticMethodID( vm_class, "dumpReferenceTables", "()V" );
env->CallStaticVoidMethod( vm_class, dump_mid );
```

## 5.5 应用信息

- 崩溃场景
- 关键操作路径
- 其它跟自身应用相关的自定义信息：运行时间、是否加载补丁、是否全新安装或升级。

接下来进行崩溃分析：

### 5.5.1 确定重点

- 确认严重程度
- 优先解决Top崩溃或对业务有重大影响的崩溃：如启动、支付过程的崩溃
- Java崩溃：如果是OOM，需进一步查看日志中的内存信息和资源信息
- Native崩溃：查看signal、code、fault addr以及崩溃时的Java堆栈

常见的崩溃类型有

SIGSEGV：空指针、非法指针等
SIGABRT：ANR、调用abort推出等

如果是ANR，先看主线程堆栈、是否因为锁等待导致，然后看ANR日志中的iowait、CPU、GC、systemserver等信息，确定是I/O问题或CPU竞争问题还是大量GC导致的ANR。

注意：

当从一条崩溃日志中无法看出问题原因时，需要查看相同崩溃点下的更多崩溃日志，或者也可以查看内存信息、资源信息等进行异常排查。

### 5.5.2 查找共性

机型、系统、ROM、厂商、ABI这些信息都可以作为共性参考，对于下一步复现问题有明确指引。

### 5.5.3 尝试复现

复现之后再增加日志或使用Debugger、GDB进行调试。如不能复现，可以采用一些高级手段，如xlog日志、远程诊断、动态分析等等。

补充：系统崩溃解决方式

- 1、通过共性信息查找可能的原因
- 2、尝试使用其它使用方式规避
- 3、Hook解决



# 6 移动端业务高可用

## 6.1 移动端容灾方案

灾包括：

- 性能异常
- 业务异常

传统流程：

用户反馈、重新打包、渠道更新、不可接受。

### 6.1.1 容灾方案建设：

**功能开关**

配置中心，服务端下发配置控制

针对场景：

- 功能新增
- 代码改动

**统跳中心**

- 界面切换通过路由，路由决定是否重定向
- Native Bug不能热修复则跳转到临时H5页面

**动态化修复**

- 热修复能力，可监控、灰度、回滚、清除

**推拉结合、多场景调用保证到达率**



### 6.1.2 天猫安全模式原理

1、如何判断异常退出？

APP启动时记录一个flag值，满足以下条件时，将flag值清空

- APP正常启动10秒
- 用户正常退出应用
- 用户主动从前台切换到后台

如果在启动阶段发生异常，则flag值不会清空，通过flag值就可以判断客户端是否异常退出，每次异常退出，flag值都+1。

2、安全模式的分级执行策略

分为两级安全模式，连续Crash 2次为一级安全模式，连续Crash 2次及以上为二级安全模式。

业务线可以在一级安全模式中注册行为，比如清空缓存数据，再进入该模式时，会使用注册行为尝试修复客户端
如果一级安全模式无法修复APP，则进入二级安全模式将APP恢复到初次安装状态，并将Document、Library、Cache三个根目录清空。

3、热修复执行策略

只要发现配置中需要热修复，APP就会同步阻塞进行热修复,保证修复的及时性

4、灰度方案

灰度时，配置中会包含灰度、正式两份配置及其灰度概率
APP根据特定算法算出自己是否满足灰度条件，则使用灰度配置



# 7 稳定性优化问题

## 7.1 你们做了哪些稳定性方面的优化？

- Crash专项优化
- 性能稳定性优化
- 业务稳定性优化

根据以上三方面的优化我们搭建了移动端的高可用平台。

## 7.2 性能稳定性是怎么做的？

- 全面的性能优化：启动速度、内存优化、绘制优化
- 线下发现问题、优化为主
- 线上监控为主
- Crash专项优化

## 7.3 业务稳定性如何保障？

- 数据采集 + 报警
- 需要对项目的主流程与核心路径进行埋点监控，
- 同时还需知道每一步发生了多少异常，这样，我们就知道了所有业务流程的转换率以及相应界面的转换率
- 结合大盘，如果转换率低于某个值，进行报警
- 异常监控 + 单点追查
- 兜底策略

## 7.4 如果发送了异常情况，怎么快速止损？

- 功能开关
- 统跳中心
- 动态修复：热修复、资源包更新
- 自主修复：安全模式



# 8 稳定性优化问题

## 8.1 业务稳定性如何保障？

- **数据采集 + 报警**
- 需要对项目的**主流程与核心路径进行埋点监控**，
- 同时还**需知道每一步发生了多少异常**，这样，我们就知道了**所有业务流程的转换率以及相应界面的转换率**
- **结合大盘，如果转换率低于某个值，进行报警**
- **异常监控 + 单点追查**
- **兜底策略，如天猫安全模式**



## 8.2 如果发生了异常情况，怎么快速止损？

- **功能开关**
- **统跳中心**
- **动态修复：热修复、资源包更新**
- **自主修复：安全模式**

首先，需要让App具备一些高级的能力，我们对于任何要上线的新功能，要加上一个功能的开关，通过配置中心下发的开关呢，来决定是否要显示新功能的入口。如果有异常情况，可以紧急关闭新功能的入口，那就可以让这个App处于可控的状态了。

然后，我们需要给App设立路由跳转，所有的界面跳转都需要通过路由来分发，如果我们匹配到需要跳转到有bug的这样一个新功能时，那我们就不跳转了，或者是跳转到统一的异常正处理中的界面。如果这两种方式都不可以，那就可以考虑通过热修复的方式来动态修复，目前热修复的方案其实已经比较成熟了，我们完全可以低成本地在我们的项目中添加热修复的能力，当然，如果有些功能是由RN或WeeX来实现就更好了，那就可以通过更新资源包的方式来实现动态更新。而这些如果都不可以的话呢，那就可以考虑自己去给应用加上一个自主修复的能力，如果App启动多次的话，那就可以考虑清空所有的缓存数据，将App重置到安装的状态，到了最严重的等级呢，可以阻塞主线程，此时一定要等App热修复成功之后才允许用户进入。


# 9 面试题
![image-20210427092748986](https://cdn.jsdelivr.net/gh/wp3355168/Typora-Picgo-Gitee/img/20210427092754.png)

# 10 参考
https://jsonchao.github.io/2019/11/24/%E6%B7%B1%E5%85%A5%E6%8E%A2%E7%B4%A2Android%E7%A8%B3%E5%AE%9A%E6%80%A7%E4%BC%98%E5%8C%96/


https://juejin.cn/post/6844903972587716621#heading-16
